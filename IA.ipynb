{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to our Music Style Classifier AI\n",
    "\n",
    "To use this AI, please execute all blocks bellow.\n",
    "Please follow the specific instructions that are written. For example, there are some cells that you don't have to execute.\n",
    "\n",
    "The last cells will allow you to detect the style of your song.\n",
    "\n",
    "### Feedbacks\n",
    "If you have any feedbacks, please let us know by posting an issue on this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pydub in c:\\users\\tom\\appdata\\roaming\\python\\python39\\site-packages (0.25.1)\n",
      "Requirement already satisfied: torch in c:\\programdata\\miniconda3\\lib\\site-packages (1.12.1+cu113)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "     ------------------------------------ 214.3/214.3 kB 687.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\miniconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.56.4-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from librosa) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\tom\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (1.9.2)\n",
      "Collecting joblib>=0.14\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "     -------------------------------------- 56.3/56.3 kB 489.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\tom\\appdata\\roaming\\python\\python39\\site-packages (from librosa) (4.4.2)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "     -------------------------------------- 377.0/377.0 kB 3.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "     ---------------------------------------- 3.1/3.1 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from librosa) (21.3)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting scikit-learn>=0.19.1\n",
      "  Downloading scikit_learn-1.2.0-cp39-cp39-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\miniconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (61.2.0)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp39-cp39-win_amd64.whl (23.2 MB)\n",
      "     ---------------------------------------- 23.2/23.2 MB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging>=20.0->librosa) (3.0.9)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.27.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\miniconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tom\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.9.24)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=3c9f9abd89c88c90ac95edf57094993d5b5b7e91f532e7b2d27faf553ae5c79d\n",
      "  Stored in directory: c:\\users\\tom\\appdata\\local\\pip\\cache\\wheels\\e4\\76\\a4\\cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "Successfully built audioread\n",
      "Installing collected packages: appdirs, threadpoolctl, llvmlite, joblib, audioread, soundfile, scikit-learn, pooch, numba, resampy, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-3.0.0 joblib-1.2.0 librosa-0.9.2 llvmlite-0.39.1 numba-0.56.4 pooch-1.6.0 resampy-0.4.2 scikit-learn-1.2.0 soundfile-0.11.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub torch librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import subprocess\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function extracts 15 seconds of a song from the \"start\" moment\n",
    "\n",
    "def feature_extractor(audio_file_dir,start):\n",
    "    \n",
    "#load the audio files\n",
    "    x,freq = librosa.load(audio_file_dir ,sr=22050)\n",
    "# trim the first 15 seconds \n",
    "    \n",
    "    x_15sec=x[22050*start:22050*(start+15)]\n",
    "#verification if the size is good\n",
    "    if len(x_15sec)!=22050*15 :\n",
    "        return False,0\n",
    "# extract 20 MFCCs\n",
    "    mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
    "# return mfcc of the first 15 sec as the audio file feature\n",
    "    return True,mfccs_15sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_dir) :\n",
    "\n",
    "    # Read file info file to get the list of audio files and their labels\n",
    "    file_list=[]\n",
    "    label_list=[]\n",
    "\n",
    "#1 Classic, 2 Rap, 3 Jazz/blues, 4 Rock, 5 Pop, 6 Electronic, 7 Ambiant.\n",
    "\n",
    "    # create a dictionary for styles\n",
    "    style_dic={'1':1,'2':2,'5':3,'6':4,'7':4,\n",
    "             '8':5,'10':6,'11':6,'12':7}\n",
    "    \n",
    "    #As we gather certain styles together, \n",
    "    #we have added an occurrence to use 150 sounds per style or group of styles\n",
    "    style_occurence={'1':1,'2':1,'5':1,'6':2,'7':2,\n",
    "             '8':1,'10':2,'11':2,'12':1}\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(data_dir):\n",
    "        \n",
    "        label=file.split(\"_\")[0]\n",
    "        if label!='3' and label!='4' and label !='9':\n",
    "            \n",
    "    \n",
    "            max_song=170/style_occurence[str(file.split(\"_\")[0])]\n",
    "            \n",
    "            if int((file.split(\"_\")[1]).split(\".\")[0])<=max_song:\n",
    "                file_list.append(file)\n",
    "                label_list.append(label)\n",
    "\n",
    "\n",
    "\n",
    "#1: 1, 2: 2, 3: 3, 4:3, 5:3, 6:4, 7: 4, 8: 2, 9: 3, 10: 5, 11: 5, 12: 6\n",
    "\n",
    "\n",
    "\n",
    "    # create a list of extracted feature (MFCC) for files\n",
    "    x_data=[]\n",
    "    for audio_file in file_list:\n",
    "\n",
    "        print(audio_file)\n",
    "        true_false,file_feature = feature_extractor(data_dir+audio_file,0)\n",
    "        #add extracted feature to dataset\n",
    "        if true_false:\n",
    "            x_data.append(file_feature)\n",
    "\n",
    "    # create a list of labels for files\n",
    "    y_data=[]\n",
    "    for style_label in label_list:\n",
    "        #convert the label to a value in {0,1,2,3....} as the class label\n",
    "        y_data.append(style_dic[style_label])\n",
    "\n",
    "    # shuffle two lists\n",
    "    temp_list = list(zip(x_data , y_data))\n",
    "    #print(x_data)\n",
    "    random.shuffle(temp_list)\n",
    "    \n",
    "    x_data , y_data = zip(*temp_list)\n",
    "\n",
    "    x_data=np.array(x_data)\n",
    "    y_data=np.array(y_data)\n",
    "    # transform to torch tensor\n",
    "    tensor_x_data = torch.Tensor(x_data)\n",
    "    tensor_y_data = torch.Tensor(y_data)\n",
    "    # create our datset\n",
    "    dataset = TensorDataset(tensor_x_data ,tensor_y_data)\n",
    "    \n",
    "    batch_size = 16\n",
    "    # create our dataloader\n",
    "    dataloader = DataLoader(dataset , batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader , model , loss_fn , optimizer):\n",
    "    \n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch , (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred , y.type(torch.LongTensor))\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad ()\n",
    "        loss.backward ()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss , current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader , model , loss_fn,list_style):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss , correct = 0, 0\n",
    "    style_pred=[]\n",
    "    with torch.no_grad ():\n",
    "        \n",
    "        #movement in the sound to test each part of the sound\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            test_loss += loss_fn(pred , y.type(torch.LongTensor)).item()\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Check if the first predicted sound is the right one\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            \n",
    "            style_pred.append(list_style[pred.argmax(1).item()-1])\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return style_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_a_song(path,name):\n",
    "    \n",
    "    #take the song that we want\n",
    "    file_path = os.path.join(path, name)\n",
    "    name = str(0) + \"_\" + str(0) + \".wav\"\n",
    "    out_path = os.path.join(path, name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the file to wav  to analize it\n",
    "    parameters = [\"ffmpeg\", \"-i\", file_path, '-ac', \"1\", '-ar', \"22050\", out_path]\n",
    "    subprocess.call(parameters,stdout=subprocess.DEVNULL,stderr=subprocess.STDOUT)\n",
    "    \n",
    "    # extract data\n",
    "    x_data=[]\n",
    "    y_data=[]\n",
    "    x,freq = librosa.load(out_path ,sr=22050)\n",
    "    \n",
    "    # take some 15 seconds parts of the song, every 15 seconds and we start at 0 seconds\n",
    "    for i in range(0,int(len(x)/freq)-15,15):\n",
    "        boolean,song=feature_extractor(path + name,i)\n",
    "        \n",
    "        #if the feature extractor went well, we add the sound part to our data\n",
    "        if boolean:\n",
    "            x_data.append(song)\n",
    "            y_data.append(0)\n",
    "        \n",
    "        \n",
    "    # transform to torch tensor\n",
    "    tensor_x_data = torch.Tensor(x_data)\n",
    "    tensor_y_data = torch.Tensor(y_data)\n",
    "    \n",
    "    # create our datset\n",
    "    dataset = TensorDataset(tensor_x_data ,tensor_y_data)\n",
    "\n",
    "    #create our data loader\n",
    "    data_loader=DataLoader(dataset , batch_size=1)\n",
    "    \n",
    "    #remove the wav song\n",
    "    os.remove(out_path)\n",
    "    \n",
    "    #return data loader\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function allow us to print and return the proportions of predicted styles\n",
    "def calcul_prop(dict):\n",
    "    size=len(dict)\n",
    "    list=np.zeros(size)\n",
    "    list_style=[]\n",
    "    n=0\n",
    "    for style in dict:\n",
    "        list[n]=dict[style]\n",
    "        n+=1\n",
    "        list_style.append(style)\n",
    "    list=list/sum(list)\n",
    "    n=0\n",
    "    for prop in list:\n",
    "        print(list_style[n]+ \" : \"+str(round(prop*100,2))+\"%\")\n",
    "        n+=1\n",
    "    return list,list_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_a_song(data_dir,file):\n",
    "    #extract data from the directory that we want to test\n",
    "    # dataloader_test_2 contains some parts of the song\n",
    "    dataloader_test_2=take_a_song(data_dir,file)\n",
    "    #styles_pred predicts the style of each part of the song\n",
    "    styles_pred=test(dataloader_test_2 , model , loss_fn,list_style)\n",
    "    print(file+\"\\n\")\n",
    "    #print the predictions in function of the time of the song\n",
    "    for j in range(0,len(styles_pred)):\n",
    "        minutes = int(j*15/60)\n",
    "        minutes2= int((j+1)*15/60)\n",
    "        seconds = j*15-60*minutes\n",
    "        seconds2= (j+1)*15-60*minutes2\n",
    "        print(str(minutes) + \":\"+str(seconds) + \" - \" + str(minutes2) + \":\"+str(seconds2) + \" : \" + styles_pred[j])\n",
    "    print(\"\")\n",
    "    #calculate the number of times a style has been predicted\n",
    "    counter=Counter(styles_pred)\n",
    "        \n",
    "    #print and take the results\n",
    "    prop,styles=calcul_prop(counter)\n",
    "    \n",
    "    #take the predicted style\n",
    "    x = max(counter, key=counter.get)\n",
    "    \n",
    "    #print predicted and expected style\n",
    "    print(\"\\nBest : \" + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork , self).__init__ ()\n",
    "        self.flatten = nn.Flatten ()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # the size of input should be the number of features (20 MFCC) times\n",
    "            # length of sequence (646)\n",
    "            nn.Linear(20*646 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512 , 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 , 13)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        \n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "list_style=['Classic','Rap',\"Jazz\",'Rock','Pop','Electronic','Ambient']\n",
    "\n",
    "#Retrieve our model\n",
    "model= NeuralNetwork()\n",
    "model.load_state_dict(torch.load('Model/model.pth'))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters (), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is where you can test your song\n",
    "\n",
    "To test your song, put it in the directory \"../Data/Example for user/\" \n",
    "and change \"RapSong.mp3\" below by your song\n",
    "\n",
    "You can modify the path to accommodate with your own architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.         0.         0.         ... 0.41818237 0.4199829  0.41921997] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.42575073 0.42059326 0.42294312 ... 0.44525146 0.43041992 0.4136963 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.41000366 0.4060974  0.41131592 ... 0.19622803 0.17700195 0.22851562] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.33569336 0.41418457 0.45565796 ... 0.42312622 0.42016602 0.4020691 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.3985901  0.38772583 0.37161255 ... 0.39813232 0.40621948 0.4048767 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[ 0.41549683  0.4072876   0.40307617 ... -0.01495361  0.01773071\n",
      " -0.08111572] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[-0.02429199 -0.04580688 -0.04190063 ... -0.42123413 -0.33840942\n",
      " -0.33874512] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[-0.190979   -0.05169678  0.02416992 ...  0.4416504   0.43597412\n",
      "  0.38619995] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.43988037 0.4345703  0.43792725 ... 0.35980225 0.4675598  0.4076538 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.48345947 0.4166565  0.43789673 ... 0.36849976 0.40429688 0.36611938] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[ 0.29315186  0.23803711  0.34848022 ... -0.01919556  0.15582275\n",
      "  0.18261719] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.43069458 0.52926636 0.509552   ... 0.29458618 0.49145508 0.40075684] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[ 0.44039917  0.45004272  0.34124756 ... -0.00054932  0.05487061\n",
      "  0.08856201] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[ 0.09094238  0.00259399 -0.03894043 ... -0.11953735  0.05352783\n",
      "  0.1317749 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.13372803 0.05123901 0.12670898 ... 0.34384155 0.3590393  0.3977356 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.46395874 0.42141724 0.3918152  ... 0.38397217 0.42651367 0.42990112] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.44595337 0.40524292 0.37646484 ... 0.29223633 0.4329834  0.4746399 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.4628296  0.41204834 0.4864807  ... 0.36810303 0.67892456 0.6079407 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[ 0.48843384  0.17285156  0.14819336 ... -0.0166626   0.10064697\n",
      "  0.24389648] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[0.38085938 0.4163208  0.5573425  ... 0.3423462  0.6480713  0.73349   ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
      "C:\\Users\\Tom\\AppData\\Local\\Temp\\ipykernel_23276\\3310081447.py:14: FutureWarning: Pass y=[ 0.5345154   0.1425476   0.11077881 ... -0.07946777 -0.03219604\n",
      "  0.0458374 ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\2727345251.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_a_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Aahan - Kingmaker - Original mix.mp3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\3950071672.py\u001b[0m in \u001b[0;36mtest_a_song\u001b[1;34m(data_dir, file)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdataloader_test_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtake_a_song\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#styles_pred predicts the style of each part of the song\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mstyles_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_test_2\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist_style\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#print the predictions in function of the time of the song\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\2024826437.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn, list_style)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\4097420096.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_relu_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mlist_style\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Classic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Rap'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Jazz\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Rock'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Pop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Electronic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Ambient'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "test_a_song(\"../\",\"Aahan - Kingmaker - Original mix.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING: DO NOT EXECUTE THE CELLS BELLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function allow us to test all the songs in a directory and give us an accuracy for our AI\n",
    "def test_all_song(data_dir_test):\n",
    "    #extract data from the directory that we want to test\n",
    "    file_list=[]\n",
    "    label_list=[]\n",
    "    style_dic={'1':1,'2':2,'5':3,'6':4,'7':4,\n",
    "             '8':5,'10':6,'11':6,'12':7}\n",
    "    \n",
    "    #browse the folder and get all the songs to test, and put their name into file_list, and their label into list_label\n",
    "    for file in os.listdir(data_dir_test):\n",
    "        \n",
    "        label=file.split(\"_\")[0]\n",
    "        if label!='3' and label!='4' and label !='9':\n",
    "            file_list.append(file)\n",
    "            label_list.append(style_dic[label])\n",
    "    \n",
    "    \n",
    "\n",
    "    #correct is to see if the song is correctly predicted\n",
    "    correct=0\n",
    "    \n",
    "    #n is to calculate the total accuracy, n will be the number total of songs analized\n",
    "    n=0\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #predict the song\n",
    "    for i in range(0,len(file_list)):\n",
    "        \n",
    "        #extract the style expected from the file name\n",
    "        try:\n",
    "            style_id=int(file_list[i][0:2])-1\n",
    "\n",
    "        except:\n",
    "            style_id=int(file_list[i][0])-1\n",
    "\n",
    "        #Display which song we will test\n",
    "        print(file_list[i])\n",
    "\n",
    "\n",
    "\n",
    "        # dataloader_test_2 contains some parts of the song\n",
    "        dataloader_test_2=take_a_song(data_dir_test,file_list[i])\n",
    "        \n",
    "        #styles_pred predicts the style of each part of the song\n",
    "        styles_pred=test(dataloader_test_2 , model , loss_fn,list_style)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #print the predictions in function of the time of the song\n",
    "        for j in range(0,len(styles_pred)):\n",
    "            minutes = int(j*15/60)\n",
    "            minutes2= int((j+1)*15/60)\n",
    "            seconds = j*15-60*minutes\n",
    "            seconds2= (j+1)*15-60*minutes2\n",
    "            print(str(minutes) + \":\"+str(seconds) + \" - \" + str(minutes2) + \":\"+str(seconds2) \n",
    "                  + \" : \" + styles_pred[j])\n",
    "            \n",
    "        \n",
    "        #calculate the number of times a style has been predicted\n",
    "        counter=Counter(styles_pred)\n",
    "        \n",
    "        #print and take the results\n",
    "        prop,styles=calcul_prop(counter)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        #take the predicted style\n",
    "        x = max(counter, key=counter.get)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        #print predicted and expected style\n",
    "        print(\"Best : \" + x)\n",
    "        print(\"Expected : \"+list_style[int(label_list[i])-1]+\"\\n \\n\")\n",
    "\n",
    "\n",
    "        n+=1\n",
    "        \n",
    "\n",
    "        \n",
    "        #add if the style was correctly predicted\n",
    "        if(x==list_style[int(label_list[i])-1]):\n",
    "            correct+=1\n",
    "            \n",
    "\n",
    "    # accuracy total\n",
    "    print(\" Accuracy : \" +str(correct/n*100))\n",
    "    \n",
    "    return correct/n*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"As a user, don't execute this\"\"\"\n",
    "#Step 1 : collect data to train our AI\n",
    "\n",
    "list_style=['Classic','Rap',\"Jazz\",'Rock','Pop','Electronic','Ambient']\n",
    "dataloader=data_loader(\"../Data/train/\")\n",
    "\n",
    "\n",
    "\n",
    "#Step 2 : initialize our neural network\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available () else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# to train a model , we need a loss function and an optimizer .\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters (), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "#Step 3 : train our neural network\n",
    "\n",
    "number_of_epochs = 24\n",
    "for t in range(number_of_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader , model , loss_fn , optimizer)\n",
    "    \n",
    "#Save our model\n",
    "torch.save(model.state_dict(), '../Model/model.pth')\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\"\"\"As a user, don't execute this\"\"\"\n",
    "#Step 4 : test our IA\n",
    "\n",
    "test_all_song(\"../Data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
