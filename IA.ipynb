{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import subprocess\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in c:\\users\\matte\\anaconda3\\lib\\site-packages (0.25.1)"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: torch in c:\\users\\matte\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\matte\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function extracts 15 seconds of a song from the \"start\" moment\n",
    "\n",
    "def feature_extractor(audio_file_dir,start):\n",
    "    \n",
    "#load the audio files\n",
    "    x,freq = librosa.load(audio_file_dir ,sr=22050)\n",
    "# trim the first 15 seconds \n",
    "    \n",
    "    x_15sec=x[22050*start:22050*(start+15)]\n",
    "#verification if the size is good\n",
    "    if len(x_15sec)!=22050*15 :\n",
    "        return False,0\n",
    "# extract 20 MFCCs\n",
    "    mfccs_15sec=librosa.feature.mfcc(x_15sec ,sr=freq ,n_mfcc=20)\n",
    "# return mfcc of the first 15 sec as the audio file feature\n",
    "    return True,mfccs_15sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def data_loader(data_dir) :\n",
    "\n",
    "    # Read file info file to get the list of audio files and their labels\n",
    "    file_list=[]\n",
    "    label_list=[]\n",
    "\n",
    "#1 Classic, 2 Rap, 3 Jazz/blues, 4 Rock, 5 Pop, 6 Electronic, 7 Ambiant.\n",
    "\n",
    "    # create a dictionary for styles\n",
    "    style_dic={'1':1,'2':2,'5':3,'6':4,'7':4,\n",
    "             '8':5,'10':6,'11':6,'12':7}\n",
    "    \n",
    "    #As we gather certain styles together, \n",
    "    #we have added an occurrence to use 150 sounds per style or group of styles\n",
    "    style_occurence={'1':1,'2':1,'5':1,'6':2,'7':2,\n",
    "             '8':1,'10':2,'11':2,'12':1}\n",
    "    \n",
    "    \n",
    "    for file in os.listdir(data_dir):\n",
    "        \n",
    "        label=file.split(\"_\")[0]\n",
    "        if label!='3' and label!='4' and label !='9':\n",
    "            \n",
    "    \n",
    "            max_song=170/style_occurence[str(file.split(\"_\")[0])]\n",
    "            \n",
    "            if int((file.split(\"_\")[1]).split(\".\")[0])<=max_song:\n",
    "                file_list.append(file)\n",
    "                label_list.append(label)\n",
    "\n",
    "\n",
    "\n",
    "#1: 1, 2: 2, 3: 3, 4:3, 5:3, 6:4, 7: 4, 8: 2, 9: 3, 10: 5, 11: 5, 12: 6\n",
    "\n",
    "\n",
    "\n",
    "    # create a list of extracted feature (MFCC) for files\n",
    "    x_data=[]\n",
    "    for audio_file in file_list:\n",
    "\n",
    "        print(audio_file)\n",
    "        true_false,file_feature = feature_extractor(data_dir+audio_file,0)\n",
    "        #add extracted feature to dataset\n",
    "        if true_false:\n",
    "            x_data.append(file_feature)\n",
    "\n",
    "    # create a list of labels for files\n",
    "    y_data=[]\n",
    "    for style_label in label_list:\n",
    "        #convert the label to a value in {0,1,2,3....} as the class label\n",
    "        y_data.append(style_dic[style_label])\n",
    "\n",
    "    # shuffle two lists\n",
    "    temp_list = list(zip(x_data , y_data))\n",
    "    #print(x_data)\n",
    "    random.shuffle(temp_list)\n",
    "    \n",
    "    x_data , y_data = zip(*temp_list)\n",
    "\n",
    "    x_data=np.array(x_data)\n",
    "    y_data=np.array(y_data)\n",
    "    # transform to torch tensor\n",
    "    tensor_x_data = torch.Tensor(x_data)\n",
    "    tensor_y_data = torch.Tensor(y_data)\n",
    "    # create our datset\n",
    "    dataset = TensorDataset(tensor_x_data ,tensor_y_data)\n",
    "    \n",
    "    batch_size = 16\n",
    "    # create our dataloader\n",
    "    dataloader = DataLoader(dataset , batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader , model , loss_fn , optimizer):\n",
    "    \n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch , (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred , y.type(torch.LongTensor))\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad ()\n",
    "        loss.backward ()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss , current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader , model , loss_fn,list_style):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss , correct = 0, 0\n",
    "    style_pred=[]\n",
    "    with torch.no_grad ():\n",
    "        \n",
    "        #movement in the sound to test each part of the sound\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            test_loss += loss_fn(pred , y.type(torch.LongTensor)).item()\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Check if the first predicted sound is the right one\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "            \n",
    "            style_pred.append(list_style[pred.argmax(1).item()-1])\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return style_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_a_song(path,name):\n",
    "    \n",
    "    #take the song that we want\n",
    "    file_path = os.path.join(path, name)\n",
    "    name = str(0) + \"_\" + str(0) + \".wav\"\n",
    "    out_path = os.path.join(path, name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Convert the file to wav  to analize it\n",
    "    parameters = [\"ffmpeg\", \"-i\", file_path, '-ac', \"1\", '-ar', \"22050\", out_path]\n",
    "    subprocess.call(parameters,stdout=subprocess.DEVNULL,stderr=subprocess.STDOUT)\n",
    "    \n",
    "    # extract data\n",
    "    x_data=[]\n",
    "    y_data=[]\n",
    "    x,freq = librosa.load(out_path ,sr=22050)\n",
    "    \n",
    "    # take some 15 seconds parts of the song, every 15 seconds and we start at 0 seconds\n",
    "    for i in range(0,int(len(x)/freq)-15,15):\n",
    "        boolean,song=feature_extractor(path + name,i)\n",
    "        \n",
    "        #if the feature extractor went well, we add the sound part to our data\n",
    "        if boolean:\n",
    "            x_data.append(song)\n",
    "            y_data.append(0)\n",
    "        \n",
    "        \n",
    "    # transform to torch tensor\n",
    "    tensor_x_data = torch.Tensor(x_data)\n",
    "    tensor_y_data = torch.Tensor(y_data)\n",
    "    \n",
    "    # create our datset\n",
    "    dataset = TensorDataset(tensor_x_data ,tensor_y_data)\n",
    "\n",
    "    #create our data loader\n",
    "    data_loader=DataLoader(dataset , batch_size=1)\n",
    "    \n",
    "    #remove the wav song\n",
    "    os.remove(out_path)\n",
    "    \n",
    "    #return data loader\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function allow us to print and return the proportions of predicted styles\n",
    "def calcul_prop(dict):\n",
    "    size=len(dict)\n",
    "    list=np.zeros(size)\n",
    "    list_style=[]\n",
    "    n=0\n",
    "    for style in dict:\n",
    "        list[n]=dict[style]\n",
    "        n+=1\n",
    "        list_style.append(style)\n",
    "    list=list/sum(list)\n",
    "    n=0\n",
    "    for prop in list:\n",
    "        print(list_style[n]+ \" : \"+str(round(prop*100,2))+\"%\")\n",
    "        n+=1\n",
    "    return list,list_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function allow us to test all the songs in a directory and give us an accuracy for our AI\n",
    "def test_all_song(data_dir_test):\n",
    "    #extract data from the directory that we want to test\n",
    "    file_list=[]\n",
    "    label_list=[]\n",
    "    style_dic={'1':1,'2':2,'5':3,'6':4,'7':4,\n",
    "             '8':5,'10':6,'11':6,'12':7}\n",
    "    \n",
    "    #browse the folder and get all the songs to test, and put their name into file_list, and their label into list_label\n",
    "    for file in os.listdir(data_dir_test):\n",
    "        \n",
    "        label=file.split(\"_\")[0]\n",
    "        if label!='3' and label!='4' and label !='9':\n",
    "            file_list.append(file)\n",
    "            label_list.append(style_dic[label])\n",
    "    \n",
    "    \n",
    "\n",
    "    #correct is to see if the song is correctly predicted\n",
    "    correct=0\n",
    "    \n",
    "    #n is to calculate the total accuracy, n will be the number total of songs analized\n",
    "    n=0\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #predict the song\n",
    "    for i in range(0,len(file_list)):\n",
    "        \n",
    "        #extract the style expected from the file name\n",
    "        try:\n",
    "            style_id=int(file_list[i][0:2])-1\n",
    "\n",
    "        except:\n",
    "            style_id=int(file_list[i][0])-1\n",
    "\n",
    "        #Display which song we will test\n",
    "        print(file_list[i])\n",
    "\n",
    "\n",
    "\n",
    "        # dataloader_test_2 contains some parts of the song\n",
    "        dataloader_test_2=take_a_song(data_dir_test,file_list[i])\n",
    "        \n",
    "        #styles_pred predicts the style of each part of the song\n",
    "        styles_pred=test(dataloader_test_2 , model , loss_fn,list_style)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #print the predictions in function of the time of the song\n",
    "        for j in range(0,len(styles_pred)):\n",
    "            minutes = int(j*15/60)\n",
    "            minutes2= int((j+1)*15/60)\n",
    "            seconds = j*15-60*minutes\n",
    "            seconds2= (j+1)*15-60*minutes2\n",
    "            print(str(minutes) + \":\"+str(seconds) + \" - \" + str(minutes2) + \":\"+str(seconds2) \n",
    "                  + \" : \" + styles_pred[j])\n",
    "            \n",
    "        \n",
    "        #calculate the number of times a style has been predicted\n",
    "        counter=Counter(styles_pred)\n",
    "        \n",
    "        #print and take the results\n",
    "        prop,styles=calcul_prop(counter)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "        #take the predicted style\n",
    "        x = max(counter, key=counter.get)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        #print predicted and expected style\n",
    "        print(\"Best : \" + x)\n",
    "        print(\"Expected : \"+list_style[int(label_list[i])-1]+\"\\n \\n\")\n",
    "\n",
    "\n",
    "        n+=1\n",
    "        \n",
    "\n",
    "        \n",
    "        #add if the style was correctly predicted\n",
    "        if(x==list_style[int(label_list[i])-1]):\n",
    "            correct+=1\n",
    "            \n",
    "\n",
    "    # accuracy total\n",
    "    print(\" Accuracy : \" +str(correct/n*100))\n",
    "    \n",
    "    return correct/n*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_a_song(data_dir,file):\n",
    "    #extract data from the directory that we want to test\n",
    "    # dataloader_test_2 contains some parts of the song\n",
    "    dataloader_test_2=take_a_song(data_dir,file)\n",
    "    #styles_pred predicts the style of each part of the song\n",
    "    styles_pred=test(dataloader_test_2 , model , loss_fn,list_style)\n",
    "    print(file+\"\\n\")\n",
    "    #print the predictions in function of the time of the song\n",
    "    for j in range(0,len(styles_pred)):\n",
    "        minutes = int(j*15/60)\n",
    "        minutes2= int((j+1)*15/60)\n",
    "        seconds = j*15-60*minutes\n",
    "        seconds2= (j+1)*15-60*minutes2\n",
    "        print(str(minutes) + \":\"+str(seconds) + \" - \" + str(minutes2) + \":\"+str(seconds2) + \" : \" + styles_pred[j])\n",
    "    print(\"\")\n",
    "    #calculate the number of times a style has been predicted\n",
    "    counter=Counter(styles_pred)\n",
    "        \n",
    "    #print and take the results\n",
    "    prop,styles=calcul_prop(counter)\n",
    "    \n",
    "    #take the predicted style\n",
    "    x = max(counter, key=counter.get)\n",
    "    \n",
    "    #print predicted and expected style\n",
    "    print(\"\\nBest : \" + x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork , self).__init__ ()\n",
    "        self.flatten = nn.Flatten ()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # the size of input should be the number of features (20 MFCC) times\n",
    "            # length of sequence (646)\n",
    "            nn.Linear(20*646 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512 , 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 , 13)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        \n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "list_style=['Classic','Rap',\"Jazz\",'Rock','Pop','Electronic','Ambient']\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available () else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "#Retrieve our model\n",
    "model= NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load('Model/model.pth'))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters (), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RapSong.mp3\n",
      "\n",
      "0:0 - 0:15 : Classic\n",
      "0:15 - 0:30 : Ambient\n",
      "0:30 - 0:45 : Ambient\n",
      "0:45 - 1:0 : Rap\n",
      "1:0 - 1:15 : Rap\n",
      "1:15 - 1:30 : Rap\n",
      "1:30 - 1:45 : Rap\n",
      "1:45 - 2:0 : Pop\n",
      "2:0 - 2:15 : Rap\n",
      "2:15 - 2:30 : Rap\n",
      "2:30 - 2:45 : Ambient\n",
      "2:45 - 3:0 : Electronic\n",
      "3:0 - 3:15 : Rap\n",
      "3:15 - 3:30 : Ambient\n",
      "3:30 - 3:45 : Jazz\n",
      "\n",
      "Classic : 6.67%\n",
      "Ambient : 26.67%\n",
      "Rap : 46.67%\n",
      "Pop : 6.67%\n",
      "Electronic : 6.67%\n",
      "Jazz : 6.67%\n",
      "\n",
      "Best : Rap\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This is where you will be able to test your music\"\"\"\n",
    "\n",
    "#Step 5 : test a specific song, this will be what the user will use to test a song\n",
    "\n",
    "#if you want to test your song, put it in the directory \"../Data/Example for user/\" \n",
    "#and change \"RapSong.mp3\" below by your song\n",
    "test_a_song(\"../Data/Example for user/\",\"RapSong.mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"As a user, don't execute this\"\"\"\n",
    "#Step 1 : collect data to train our AI\n",
    "\n",
    "list_style=['Classic','Rap',\"Jazz\",'Rock','Pop','Electronic','Ambient']\n",
    "dataloader=data_loader(\"../Data/train/\")\n",
    "\n",
    "\n",
    "\n",
    "#Step 2 : initialize our neural network\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available () else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# to train a model , we need a loss function and an optimizer .\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters (), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "#Step 3 : train our neural network\n",
    "\n",
    "number_of_epochs = 24\n",
    "for t in range(number_of_epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader , model , loss_fn , optimizer)\n",
    "    \n",
    "#Save our model\n",
    "torch.save(model.state_dict(), '../Model/model.pth')\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\"\"\"As a user, don't execute this\"\"\"\n",
    "#Step 4 : test our IA\n",
    "\n",
    "test_all_song(\"../Data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
