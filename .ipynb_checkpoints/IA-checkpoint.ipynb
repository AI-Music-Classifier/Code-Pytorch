{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The duration of 0_0000.wav in seconds: 10.0\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "# sr should be set to your recording sample rate (16k)\n",
    "x,freq = librosa.load(\"C:/Users/matte/Desktop/IA/HIP_HOP_MUSIC/0_0000.wav\",sr=16000)\n",
    "# The load function will return a time series value (x) and\n",
    "# the input sample rate (freq) which is 16000\n",
    "print(\"The duration of 0_0000.wav in seconds:\",len(x)/freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The duration of 0_0000.wav in seconds: 10.0\n"
     ]
    }
   ],
   "source": [
    "x=x[:160000]\n",
    "print(\"The duration of 0_0000.wav in seconds:\",len(x)/freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 313)\n"
     ]
    }
   ],
   "source": [
    "# This function will return n_mfcc number of MFCC per\n",
    "# a window of time in audio time series\n",
    "x_mfcc=librosa.feature.mfcc(x,sr=freq , n_mfcc=40)\n",
    "print(x_mfcc.shape)\n",
    "\n",
    "# x_mfcc is an array with 40 values for a window of time\n",
    "# The len(x_mfcc) is a proportion of wav file duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(audio_file_dir):\n",
    "#load the audio files\n",
    "    x,freq = librosa.load(audio_file_dir ,sr=16000)\n",
    "# trim the first 5 seconds (Sequence Truncation )\n",
    "    x_10sec=x[:160000]\n",
    "# extract 20 MFCCs\n",
    "    mfccs_10sec=librosa.feature.mfcc(x_10sec ,sr=freq ,n_mfcc=20)\n",
    "# return mfcc of the first 5 sec as the audio file feature\n",
    "    return mfccs_10sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_0000.wav\n",
      "0_0001.wav\n",
      "0_0002.wav\n",
      "0_0003.wav\n",
      "0_0004.wav\n",
      "0_0005.wav\n",
      "0_0006.wav\n",
      "0_0007.wav\n",
      "0_0008.wav\n",
      "0_0009.wav\n",
      "0_0010.wav\n",
      "0_0011.wav\n",
      "0_0012.wav\n",
      "0_0013.wav\n",
      "0_0014.wav\n",
      "0_0015.wav\n",
      "0_0016.wav\n",
      "0_0017.wav\n",
      "0_0018.wav\n",
      "0_0019.wav\n",
      "0_0020.wav\n",
      "0_0021.wav\n",
      "0_0022.wav\n",
      "0_0023.wav\n",
      "0_0024.wav\n",
      "0_0025.wav\n",
      "0_0026.wav\n",
      "0_0027.wav\n",
      "0_0028.wav\n",
      "0_0029.wav\n",
      "0_0030.wav\n",
      "0_0031.wav\n",
      "0_0032.wav\n",
      "0_0033.wav\n",
      "0_0034.wav\n",
      "0_0035.wav\n",
      "0_0036.wav\n",
      "0_0037.wav\n",
      "0_0038.wav\n",
      "0_0039.wav\n",
      "0_0040.wav\n",
      "0_0041.wav\n",
      "0_0042.wav\n",
      "0_0043.wav\n",
      "0_0044.wav\n",
      "0_0045.wav\n",
      "0_0046.wav\n",
      "0_0047.wav\n",
      "0_0048.wav\n",
      "0_0049.wav\n",
      "0_0050.wav\n",
      "0_0051.wav\n",
      "0_0052.wav\n",
      "0_0053.wav\n",
      "0_0054.wav\n",
      "0_0055.wav\n",
      "0_0056.wav\n",
      "0_0057.wav\n",
      "0_0058.wav\n",
      "0_0059.wav\n",
      "0_0060.wav\n",
      "0_0061.wav\n",
      "0_0062.wav\n",
      "0_0063.wav\n",
      "0_0064.wav\n",
      "0_0065.wav\n",
      "0_0066.wav\n",
      "0_0067.wav\n",
      "0_0068.wav\n",
      "0_0069.wav\n",
      "0_0070.wav\n",
      "0_0071.wav\n",
      "0_0072.wav\n",
      "0_0073.wav\n",
      "0_0074.wav\n",
      "0_0075.wav\n",
      "0_0076.wav\n",
      "0_0077.wav\n",
      "0_0078.wav\n",
      "0_0079.wav\n",
      "0_0080.wav\n",
      "0_0081.wav\n",
      "0_0082.wav\n",
      "0_0083.wav\n",
      "0_0084.wav\n",
      "0_0085.wav\n",
      "0_0086.wav\n",
      "0_0087.wav\n",
      "0_0088.wav\n",
      "0_0089.wav\n",
      "0_0090.wav\n",
      "0_0091.wav\n",
      "0_0092.wav\n",
      "0_0093.wav\n",
      "0_0094.wav\n",
      "0_0095.wav\n",
      "0_0096.wav\n",
      "0_0097.wav\n",
      "0_0098.wav\n",
      "0_0099.wav\n",
      "0_0100.wav\n",
      "1_0000.wav\n",
      "1_0001.wav\n",
      "1_0002.wav\n",
      "1_0003.wav\n",
      "1_0004.wav\n",
      "1_0005.wav\n",
      "1_0006.wav\n",
      "1_0007.wav\n",
      "1_0008.wav\n",
      "1_0009.wav\n",
      "1_0010.wav\n",
      "1_0011.wav\n",
      "1_0012.wav\n",
      "1_0013.wav\n",
      "1_0014.wav\n",
      "1_0015.wav\n",
      "1_0016.wav\n",
      "1_0017.wav\n",
      "1_0018.wav\n",
      "1_0019.wav\n",
      "1_0020.wav\n",
      "1_0021.wav\n",
      "1_0023.wav\n",
      "1_0024.wav\n",
      "1_0025.wav\n",
      "1_0026.wav\n",
      "1_0027.wav\n",
      "1_0028.wav\n",
      "1_0029.wav\n",
      "1_0030.wav\n",
      "1_0031.wav\n",
      "1_0032.wav\n",
      "1_0033.wav\n",
      "1_0034.wav\n",
      "1_0035.wav\n",
      "1_0036.wav\n",
      "1_0037.wav\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/HIP_HOP_MUSIC/\"\n",
    "\n",
    "# Read file info file to get the list of audio files and their labels\n",
    "file_list=[]\n",
    "label_list=[]\n",
    "\n",
    "with open(data_dir+\"info.txt\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # The first column contains the file name\n",
    "        file_list.append(row[0])\n",
    "        # The last column contains the lable (language)\n",
    "        label_list.append(row[0][0])\n",
    "\n",
    "#set data_dir to the directory of your data files\n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/CLASSIC_MUSIC/\"\n",
    "\n",
    "with open(data_dir+\"info.txt\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # The first column contains the file name\n",
    "        file_list.append(row[0])\n",
    "        # The last column contains the lable (language)\n",
    "        label_list.append(row[0][0])\n",
    "\n",
    "        \n",
    "        \n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/HIP_HOP_MUSIC/\"\n",
    "# create a dictionary for labels\n",
    "lang_dic={'0':0,'1':1,'2':2,'3':3,}\n",
    "# create a list of extracted feature (MFCC) for files\n",
    "x_data=[]\n",
    "for audio_file in file_list:\n",
    "    if audio_file[0]=='1':\n",
    "        data_dir= \"C:/Users/matte/Desktop/IA/Data/CLASSIC_MUSIC/\"\n",
    "        \n",
    "    print(audio_file)\n",
    "    file_feature = feature_extractor(data_dir+audio_file)\n",
    "    #add extracted feature to dataset\n",
    "    x_data.append(file_feature)\n",
    "    \n",
    "# create a list of labels for files\n",
    "y_data=[]\n",
    "for lang_label in label_list:\n",
    "    #convert the label to a value in {0,1,2,3} as the class label\n",
    "    #print(lang_dic[lang_label])\n",
    "    y_data.append(lang_dic[lang_label])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shuffle two lists\n",
    "temp_list = list(zip(x_data , y_data))\n",
    "random.shuffle(temp_list)\n",
    "x_data , y_data = zip(*temp_list)\n",
    "\n",
    "x_data=np.array(x_data)\n",
    "y_data=np.array(y_data)\n",
    "# transform to torch tensor\n",
    "tensor_x_data = torch.Tensor(x_data)\n",
    "tensor_y_data = torch.Tensor(y_data)\n",
    "# create your datset\n",
    "dataset = TensorDataset(tensor_x_data ,tensor_y_data)\n",
    "# the batch size can be changed to a lrger value when you have more data\n",
    "batch_size = 1\n",
    "# create your dataloader\n",
    "dataloader = DataLoader(dataset , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([138, 20, 313]) torch.Size([138])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_x_data.shape,tensor_y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\matte\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\matte\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available () else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork , self).__init__ ()\n",
    "        self.flatten = nn.Flatten ()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # the size of input should be the number of features (20 MFCC) times\n",
    "            # length of sequence (313)\n",
    "            nn.Linear(20*313 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512 , 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512 , 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256 , 4)\n",
    "        )\n",
    "    def forward(self , x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        \n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "model = NeuralNetwork().to(device)\n",
    "# to train a model , we need a loss function and an optimizer .\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters (), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pred):\n",
    "    #This fonction allows to derterminate proportion of probability the song is\n",
    "    prop_hip_hop=pred[0][0]\n",
    "    prop_classic=pred[0][1]\n",
    "    div=0.\n",
    "    for i in pred[0]:\n",
    "        if i > 0:\n",
    "            div+=i\n",
    "    \n",
    "    if prop_hip_hop<0:\n",
    "        prop_hip_hop=0\n",
    "    if prop_classic<0:\n",
    "        prop_classic=0\n",
    "    \n",
    "    average=[(prop_hip_hop/div*100).item(),(prop_classic/div*100).item()]\n",
    "    \n",
    "    return average\n",
    "\n",
    "def train(dataloader , model , loss_fn , optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train ()\n",
    "    for batch , (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred , y.type(torch.LongTensor))\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad ()\n",
    "        loss.backward ()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss , current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "def test(dataloader , model , loss_fn,list_style):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss , correct = 0, 0\n",
    "    with torch.no_grad ():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            \n",
    "            test_loss += loss_fn(pred , y.type(torch.LongTensor)).item()\n",
    "            \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            prop_HH,prop_CL=evaluate(pred)\n",
    "            print('Prediction '+list_style[0] +' : '+ str(prop_HH) +\n",
    "                  '%\\nPrediction '+list_style[1] +' : '+ str(prop_CL) +\n",
    "                  '%\\nExpect : ' + list_style[int(y[0])] + \"\\n\\n\")\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.009457 [    0/  138]\n",
      "loss: 0.022748 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000544 [    0/  138]\n",
      "loss: 0.001805 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000082 [    0/  138]\n",
      "loss: 0.002151 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.000370 [    0/  138]\n",
      "loss: 0.000224 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000069 [    0/  138]\n",
      "loss: 0.000110 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.000033 [    0/  138]\n",
      "loss: 0.000057 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000017 [    0/  138]\n",
      "loss: 0.000038 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.000011 [    0/  138]\n",
      "loss: 0.000028 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000008 [    0/  138]\n",
      "loss: 0.000022 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000006 [    0/  138]\n",
      "loss: 0.000018 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000005 [    0/  138]\n",
      "loss: 0.000015 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.000004 [    0/  138]\n",
      "loss: 0.000013 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.000003 [    0/  138]\n",
      "loss: 0.000011 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.000003 [    0/  138]\n",
      "loss: 0.000010 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.000003 [    0/  138]\n",
      "loss: 0.000009 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000002 [    0/  138]\n",
      "loss: 0.000008 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.000002 [    0/  138]\n",
      "loss: 0.000008 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.000002 [    0/  138]\n",
      "loss: 0.000007 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.000002 [    0/  138]\n",
      "loss: 0.000006 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000006 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000005 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000005 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000005 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000005 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000004 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000004 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000004 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000004 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.000001 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000003 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "CrossEntropyLoss()\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000000 [    0/  138]\n",
      "loss: 0.000002 [  100/  138]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "number_of_epochs = 50\n",
    "for t in range(number_of_epochs):\n",
    "    #print(model)\n",
    "    #print(loss_fn)\n",
    "    #print(optimizer)\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader , model , loss_fn , optimizer)\n",
    "    #test(dataloader , model , loss_fn)\n",
    "    \n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1\n",
      " 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_0000.wav\n",
      "0_0001.wav\n",
      "0_0002.wav\n",
      "0_0003.wav\n",
      "0_0004.wav\n",
      "0_0005.wav\n",
      "0_0006.wav\n",
      "0_0007.wav\n",
      "0_0008.wav\n",
      "0_0009.wav\n",
      "0_0010.wav\n",
      "1_0000.wav\n",
      "1_0001.wav\n",
      "1_0002.wav\n",
      "1_0003.wav\n",
      "1_0004.wav\n",
      "1_0005.wav\n",
      "1_0006.wav\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset , DataLoader\n",
    "import torch\n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/SONG_TEST/\"\n",
    "\n",
    "# Read file info file to get the list of audio files and their labels\n",
    "file_list=[]\n",
    "label_list=[]\n",
    "\n",
    "with open(data_dir+\"info.txt\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # The first column contains the file name\n",
    "        file_list.append(row[0])\n",
    "        # The last column contains the lable (language)\n",
    "        label_list.append(row[0][0])\n",
    "\n",
    "\n",
    "        \n",
    "# create a dictionary for labels\n",
    "lang_dic={'0':0,'1':1,'2':2,'3':3,}\n",
    "# create a list of extracted feature (MFCC) for files\n",
    "x_data_test=[]\n",
    "for audio_file in file_list:\n",
    "    \n",
    "    print(audio_file)\n",
    "    file_feature = feature_extractor(data_dir+audio_file)\n",
    "    #add extracted feature to dataset\n",
    "    x_data_test.append(file_feature)\n",
    "    \n",
    "# create a list of labels for files\n",
    "y_data_test=[]\n",
    "for lang_label in label_list:\n",
    "    #convert the label to a value in {0,1,2,3} as the class label\n",
    "    #print(lang_dic[lang_label])\n",
    "    y_data_test.append(lang_dic[lang_label])\n",
    "    \n",
    "    \n",
    "# shuffle two lists\n",
    "temp_list = list(zip(x_data_test , y_data_test))\n",
    "random.shuffle(temp_list)\n",
    "x_data_test , y_data_test = zip(*temp_list)\n",
    "x_data_test=np.array(x_data_test)\n",
    "y_data_test=np.array(y_data_test)\n",
    "# transform to torch tensor\n",
    "tensor_x_data_test = torch.Tensor(x_data_test)\n",
    "tensor_y_data_test = torch.Tensor(y_data_test)\n",
    "# create your datset\n",
    "dataset_test = TensorDataset(tensor_x_data_test ,tensor_y_data_test)\n",
    "# the batch size can be changed to a lrger value when you have more data\n",
    "batch_size = 1\n",
    "# create your dataloader\n",
    "dataloader_test = DataLoader(dataset_test , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Hip-Hop : 66.8038330078125%\n",
      "Prediction Classic : 33.1961669921875%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 44.35721206665039%\n",
      "Prediction Classic : 55.64278793334961%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 3.4986813068389893%\n",
      "Prediction Classic : 96.5013198852539%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 97.21685791015625%\n",
      "Prediction Classic : 2.7831430435180664%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 46.88935470581055%\n",
      "Prediction Classic : 53.11064147949219%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 98.79155731201172%\n",
      "Prediction Classic : 1.2084378004074097%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 6.164891719818115%\n",
      "Prediction Classic : 93.8351058959961%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 40.63009262084961%\n",
      "Prediction Classic : 59.369903564453125%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 40.78680419921875%\n",
      "Prediction Classic : 59.21319580078125%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 100.0%\n",
      "Prediction Classic : 0.0%\n",
      "Expect : Hip-Hop\n",
      "\n",
      "\n",
      "Prediction Hip-Hop : 65.11299896240234%\n",
      "Prediction Classic : 34.88700866699219%\n",
      "Expect : Classic\n",
      "\n",
      "\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.699892 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_style=['Hip-Hop','Classic']\n",
    "test(dataloader_test , model , loss_fn, list_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NE PAS EXECUTER CE CODE, MAIS CELUI D'APRES (fin vous pouvez mais ça sert à rien)\n",
    "#\n",
    "#\n",
    "#On garde ce code car c'est une autre façon de faire\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "def feature_extractor(audio_file_dir):\n",
    "    #load the audio files\n",
    "    x,freq = librosa.load(audio_file_dir ,sr=16000)\n",
    "    #extract 20 MFCCs\n",
    "    mfcc=librosa.feature.mfcc(x,sr=freq ,n_mfcc=20)\n",
    "    # calculate the mean and variance of each MFFC\n",
    "    mean_mfccs=np.mean(mfcc ,axis=1)\n",
    "    var_mfccs=np.var(mfcc ,axis=1)\n",
    "    #return mean and variance as the audio file feature\n",
    "    return list(mean_mfccs)+list(var_mfccs)\n",
    "\n",
    "#set data_dir to the directory of your data files\n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/HIP_HOP_MUSIC/\"\n",
    "\n",
    "# Read file info file to get the list of audio files and their labels\n",
    "file_list=[]\n",
    "label_list=[]\n",
    "\n",
    "\n",
    "with open(data_dir+\"info.txt\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # The first column contains the file name\n",
    "        file_list.append(row[0])\n",
    "        # The last column contains the lable (language)\n",
    "        label_list.append(row[0][0])\n",
    "\n",
    "#set data_dir to the directory of your data files\n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/CLASSIC_MUSIC/\"\n",
    "\n",
    "with open(data_dir+\"info.txt\", 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # The first column contains the file name\n",
    "        file_list.append(row[0])\n",
    "        # The last column contains the lable (language)\n",
    "        label_list.append(row[0][0])\n",
    "\n",
    "        \n",
    "        \n",
    "data_dir= \"C:/Users/matte/Desktop/IA/Data/HIP_HOP_MUSIC/\"\n",
    "# create a dictionary for labels\n",
    "lang_dic={'0':0,'1':1,'2':2,'3':3,}\n",
    "# create a list of extracted feature (MFCC) for files\n",
    "x_data=[]\n",
    "for audio_file in file_list:\n",
    "    \n",
    "    if audio_file[0]=='1':\n",
    "        data_dir= \"C:/Users/matte/Desktop/IA/Data/CLASSIC_MUSIC/\"\n",
    "        \n",
    "    \n",
    "    file_feature = feature_extractor(data_dir+audio_file)\n",
    "    #add extracted feature to dataset\n",
    "    x_data.append(file_feature)\n",
    "    \n",
    "# create a list of labels for files\n",
    "y_data=[]\n",
    "for lang_label in label_list:\n",
    "    #convert the label to a value in {0,1,2,3} as the class label\n",
    "    print(lang_dic[lang_label])\n",
    "    y_data.append(lang_dic[lang_label])\n",
    "    \n",
    "    \n",
    "# shuffle two lists\n",
    "temp_list = list(zip(x_data , y_data))\n",
    "random.shuffle(temp_list)\n",
    "x_data , y_data = zip(*temp_list)\n",
    "# create Random Forest model from sklearn to classify languages of audio files\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "x_test = x_data[:len(x_data) // 8]\n",
    "y_test = y_data[:len(y_data) // 8]\n",
    "\n",
    "x_train = x_data[len(x_data) // 8:]\n",
    "y_train = y_data[len(y_data) // 8:]\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# the resulted accuracy is on a small set which is same for train and test\n",
    "print(\"Accuracy\",clf.score(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
